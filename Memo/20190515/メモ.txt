
1000 step程度で学習が飽和しているようにみえる。
num_of_neuron = 200, rnn = BasicLSTMCell
学習率はデフォルト(0.001?)

0.001 -> 0.0001
学習率を下げて、再評価したところ、学習の進みが遅くなった。それと同時に、
収束点の品質も下がってしまった、、、局所解に陥った？

0.001 -> 0.01
学習率をあげてみた。バタバタするかと思いきや、そんなこともない。
下記にはないが、購入回数も多い。しかし、利益には繋がらない、、、

learning_rate = 0.01
2019/05/16 21:37:05: step     0, loss 0.984, acc 0.958, std 0.247
2019/05/16 21:38:45: step    50, loss 0.036, acc 0.879, std 0.054
2019/05/16 21:40:27: step   100, loss 0.019, acc 0.942, std 0.052
2019/05/16 21:42:07: step   150, loss 0.007, acc 0.929, std 0.054
2019/05/16 21:43:47: step   200, loss 0.009, acc 0.954, std 0.048
2019/05/16 21:45:26: step   250, loss 0.005, acc 0.945, std 0.041
2019/05/16 21:47:06: step   300, loss 0.014, acc 0.940, std 0.046
2019/05/16 21:48:46: step   350, loss 0.008, acc 0.971, std 0.037
2019/05/16 21:50:26: step   400, loss 0.007, acc 0.979, std 0.034
2019/05/16 21:52:05: step   450, loss 0.007, acc 0.963, std 0.041
2019/05/16 21:53:44: step   500, loss 0.009, acc 0.933, std 0.042
2019/05/16 21:55:23: step   550, loss 0.006, acc 0.976, std 0.031
2019/05/16 21:57:01: step   600, loss 0.006, acc 0.960, std 0.032
2019/05/16 21:58:40: step   650, loss 0.004, acc 0.966, std 0.029
2019/05/16 22:00:18: step   700, loss 0.006, acc 0.976, std 0.035
2019/05/16 22:01:56: step   750, loss 0.006, acc 0.958, std 0.043
2019/05/16 22:03:35: step   800, loss 0.006, acc 0.971, std 0.033
2019/05/16 22:05:13: step   850, loss 0.004, acc 0.967, std 0.040
2019/05/16 22:06:52: step   900, loss 0.003, acc 0.978, std 0.029
2019/05/16 22:08:31: step   950, loss 0.005, acc 0.954, std 0.032
2019/05/16 22:10:09: step  1000, loss 0.006, acc 0.966, std 0.037
2019/05/16 22:11:48: step  1050, loss 0.010, acc 0.974, std 0.030
2019/05/16 22:13:26: step  1100, loss 0.004, acc 0.984, std 0.030
2019/05/16 22:15:04: step  1150, loss 0.004, acc 0.984, std 0.025
2019/05/16 22:16:43: step  1200, loss 0.004, acc 0.997, std 0.022
2019/05/16 22:18:22: step  1250, loss 0.008, acc 0.983, std 0.026
2019/05/16 22:20:00: step  1300, loss 0.006, acc 0.981, std 0.029
2019/05/16 22:21:39: step  1350, loss 0.005, acc 0.980, std 0.027
2019/05/16 22:23:17: step  1400, loss 0.003, acc 0.985, std 0.030
2019/05/16 22:24:57: step  1450, loss 0.004, acc 0.974, std 0.027
2019/05/16 22:26:36: step  1500, loss 0.005, acc 0.997, std 0.029
2019/05/16 22:28:15: step  1550, loss 0.004, acc 0.979, std 0.029
2019/05/16 22:29:54: step  1600, loss 0.002, acc 0.986, std 0.022
2019/05/16 22:31:33: step  1650, loss 0.006, acc 0.993, std 0.025
2019/05/16 22:33:11: step  1700, loss 0.009, acc 0.963, std 0.041
2019/05/16 22:34:50: step  1750, loss 0.003, acc 0.980, std 0.025
2019/05/16 22:36:29: step  1800, loss 0.010, acc 1.007, std 0.024
2019/05/16 22:38:08: step  1850, loss 0.009, acc 1.002, std 0.026
2019/05/16 22:39:47: step  1900, loss 0.004, acc 0.981, std 0.029
2019/05/16 22:41:26: step  1950, loss 0.006, acc 0.990, std 0.030
2019/05/16 22:43:04: step  2000, loss 0.005, acc 0.986, std 0.030
2019/05/16 22:44:43: step  2050, loss 0.005, acc 0.983, std 0.028
2019/05/16 22:46:21: step  2100, loss 0.006, acc 1.004, std 0.026
2019/05/16 22:48:00: step  2150, loss 0.004, acc 0.990, std 0.025
2019/05/16 22:49:39: step  2200, loss 0.003, acc 0.985, std 0.026
2019/05/16 22:51:18: step  2250, loss 0.011, acc 0.991, std 0.026
2019/05/16 22:52:57: step  2300, loss 0.004, acc 0.965, std 0.038
2019/05/16 22:54:36: step  2350, loss 0.004, acc 0.993, std 0.033
2019/05/16 22:56:15: step  2400, loss 0.007, acc 0.997, std 0.025
2019/05/16 22:57:54: step  2450, loss 0.004, acc 1.001, std 0.026
2019/05/16 22:59:32: step  2500, loss 0.005, acc 1.003, std 0.033
2019/05/16 23:01:11: step  2550, loss 0.005, acc 0.984, std 0.031
2019/05/16 23:02:49: step  2600, loss 0.013, acc 0.976, std 0.038
2019/05/16 23:04:28: step  2650, loss 0.004, acc 0.981, std 0.033
2019/05/16 23:06:07: step  2700, loss 0.012, acc 0.979, std 0.025
2019/05/16 23:07:45: step  2750, loss 0.002, acc 0.985, std 0.029
2019/05/16 23:09:23: step  2800, loss 0.007, acc 0.981, std 0.054
2019/05/16 23:11:02: step  2850, loss 0.004, acc 0.952, std 0.059
2019/05/16 23:12:40: step  2900, loss 0.004, acc 0.987, std 0.039
2019/05/16 23:14:19: step  2950, loss 0.005, acc 0.990, std 0.026
2019/05/16 23:15:57: step  3000, loss 0.003, acc 0.978, std 0.028
2019/05/16 23:17:37: step  3050, loss 0.004, acc 0.998, std 0.025
2019/05/16 23:19:16: step  3100, loss 0.005, acc 0.986, std 0.023
2019/05/16 23:20:54: step  3150, loss 0.005, acc 0.997, std 0.034
2019/05/16 23:22:33: step  3200, loss 0.004, acc 0.986, std 0.031
2019/05/16 23:24:12: step  3250, loss 0.002, acc 0.995, std 0.032
2019/05/16 23:25:51: step  3300, loss 0.004, acc 0.965, std 0.036
2019/05/16 23:27:30: step  3350, loss 0.004, acc 0.964, std 0.033
2019/05/16 23:29:09: step  3400, loss 0.006, acc 0.983, std 0.024
2019/05/16 23:30:47: step  3450, loss 0.006, acc 0.993, std 0.041
2019/05/16 23:32:26: step  3500, loss 0.003, acc 0.975, std 0.036
2019/05/16 23:34:05: step  3550, loss 0.004, acc 0.975, std 0.041
2019/05/16 23:35:43: step  3600, loss 0.002, acc 0.992, std 0.033
2019/05/16 23:37:22: step  3650, loss 0.004, acc 0.964, std 0.046
2019/05/16 23:39:01: step  3700, loss 0.003, acc 0.992, std 0.038
2019/05/16 23:40:40: step  3750, loss 0.004, acc 0.959, std 0.048
2019/05/16 23:42:19: step  3800, loss 0.002, acc 0.962, std 0.042
2019/05/16 23:43:58: step  3850, loss 0.003, acc 0.986, std 0.032
2019/05/16 23:45:37: step  3900, loss 0.003, acc 0.979, std 0.042
2019/05/16 23:47:16: step  3950, loss 0.004, acc 0.992, std 0.034
2019/05/16 23:48:54: step  4000, loss 0.004, acc 0.984, std 0.039

learning_rate = 0.001(default)
2019/05/14 21:04:35: step     0, loss 0.752, acc 0.958, std 0.247
2019/05/14 21:06:16: step    50, loss 0.022, acc 0.894, std 0.080
2019/05/14 21:07:57: step   100, loss 0.011, acc 0.913, std 0.067
2019/05/14 21:09:37: step   150, loss 0.010, acc 0.930, std 0.056
2019/05/14 21:11:17: step   200, loss 0.011, acc 0.927, std 0.061
2019/05/14 21:12:57: step   250, loss 0.014, acc 0.947, std 0.057
2019/05/14 21:14:37: step   300, loss 0.010, acc 0.967, std 0.042
2019/05/14 21:16:17: step   350, loss 0.008, acc 0.955, std 0.043
2019/05/14 21:17:57: step   400, loss 0.008, acc 0.963, std 0.045
2019/05/14 21:19:37: step   450, loss 0.012, acc 0.967, std 0.040
2019/05/14 21:21:16: step   500, loss 0.007, acc 0.958, std 0.042
2019/05/14 21:22:55: step   550, loss 0.006, acc 0.980, std 0.037
2019/05/14 21:24:35: step   600, loss 0.004, acc 0.961, std 0.037
2019/05/14 21:26:14: step   650, loss 0.007, acc 0.966, std 0.037
2019/05/14 21:27:54: step   700, loss 0.009, acc 0.972, std 0.034
2019/05/14 21:29:33: step   750, loss 0.005, acc 0.974, std 0.034
2019/05/14 21:31:12: step   800, loss 0.007, acc 0.957, std 0.035
2019/05/14 21:32:52: step   850, loss 0.006, acc 0.962, std 0.039
2019/05/14 21:34:31: step   900, loss 0.005, acc 0.975, std 0.034
2019/05/14 21:36:10: step   950, loss 0.005, acc 0.972, std 0.034
2019/05/14 21:37:49: step  1000, loss 0.004, acc 0.965, std 0.035
2019/05/14 21:39:28: step  1050, loss 0.006, acc 0.952, std 0.044
2019/05/14 21:41:07: step  1100, loss 0.005, acc 0.966, std 0.034
2019/05/14 21:42:46: step  1150, loss 0.003, acc 0.973, std 0.035
2019/05/14 21:44:25: step  1200, loss 0.005, acc 0.971, std 0.028
2019/05/14 21:46:03: step  1250, loss 0.004, acc 0.971, std 0.034
2019/05/14 21:47:42: step  1300, loss 0.008, acc 0.973, std 0.028
2019/05/14 21:49:22: step  1350, loss 0.003, acc 0.957, std 0.039
2019/05/14 21:51:01: step  1400, loss 0.006, acc 0.971, std 0.034
2019/05/14 21:52:40: step  1450, loss 0.004, acc 0.975, std 0.033
2019/05/14 21:54:19: step  1500, loss 0.005, acc 0.962, std 0.032
2019/05/14 21:55:59: step  1550, loss 0.004, acc 0.961, std 0.038
2019/05/14 21:57:38: step  1600, loss 0.002, acc 0.976, std 0.028
2019/05/14 21:59:16: step  1650, loss 0.006, acc 0.971, std 0.027
2019/05/14 22:00:56: step  1700, loss 0.017, acc 0.953, std 0.034
2019/05/14 22:02:35: step  1750, loss 0.008, acc 0.963, std 0.029
2019/05/14 22:04:14: step  1800, loss 0.003, acc 0.967, std 0.035
2019/05/14 22:05:53: step  1850, loss 0.004, acc 0.959, std 0.035
2019/05/14 22:07:32: step  1900, loss 0.007, acc 0.964, std 0.034
2019/05/14 22:09:11: step  1950, loss 0.004, acc 0.951, std 0.038
2019/05/14 22:10:50: step  2000, loss 0.004, acc 0.970, std 0.037
2019/05/14 22:12:29: step  2050, loss 0.003, acc 0.962, std 0.039
2019/05/14 22:14:08: step  2100, loss 0.003, acc 0.974, std 0.037
2019/05/14 22:15:47: step  2150, loss 0.004, acc 0.950, std 0.039
2019/05/14 22:17:27: step  2200, loss 0.002, acc 0.962, std 0.036
2019/05/14 22:19:06: step  2250, loss 0.002, acc 0.969, std 0.034
2019/05/14 22:20:45: step  2300, loss 0.006, acc 0.967, std 0.035
2019/05/14 22:22:24: step  2350, loss 0.005, acc 0.969, std 0.034
2019/05/14 22:24:04: step  2400, loss 0.004, acc 0.966, std 0.037
2019/05/14 22:25:43: step  2450, loss 0.003, acc 0.975, std 0.036
2019/05/14 22:27:22: step  2500, loss 0.004, acc 0.965, std 0.035
2019/05/14 22:29:01: step  2550, loss 0.004, acc 0.967, std 0.038
2019/05/14 22:30:42: step  2600, loss 0.004, acc 0.977, std 0.032
2019/05/14 22:32:21: step  2650, loss 0.002, acc 0.970, std 0.033
2019/05/14 22:34:00: step  2700, loss 0.003, acc 0.967, std 0.040
2019/05/14 22:35:39: step  2750, loss 0.004, acc 0.964, std 0.043
2019/05/14 22:37:18: step  2800, loss 0.004, acc 0.972, std 0.034
2019/05/14 22:38:57: step  2850, loss 0.004, acc 0.968, std 0.035
2019/05/14 22:40:36: step  2900, loss 0.004, acc 0.964, std 0.037
2019/05/14 22:42:16: step  2950, loss 0.003, acc 0.964, std 0.042
2019/05/14 22:43:55: step  3000, loss 0.004, acc 0.966, std 0.033
2019/05/14 22:45:34: step  3050, loss 0.004, acc 0.977, std 0.034
2019/05/14 22:47:13: step  3100, loss 0.002, acc 0.974, std 0.034
2019/05/14 22:48:53: step  3150, loss 0.002, acc 0.969, std 0.031
2019/05/14 22:50:32: step  3200, loss 0.010, acc 0.967, std 0.029
2019/05/14 22:52:11: step  3250, loss 0.003, acc 0.964, std 0.035
2019/05/14 22:53:50: step  3300, loss 0.004, acc 0.958, std 0.040
2019/05/14 22:55:29: step  3350, loss 0.003, acc 0.962, std 0.038
2019/05/14 22:57:09: step  3400, loss 0.005, acc 0.983, std 0.027
2019/05/14 22:58:48: step  3450, loss 0.004, acc 0.970, std 0.031
2019/05/14 23:00:28: step  3500, loss 0.004, acc 0.973, std 0.033
2019/05/14 23:02:08: step  3550, loss 0.002, acc 0.966, std 0.041
2019/05/14 23:03:47: step  3600, loss 0.003, acc 0.957, std 0.034
2019/05/14 23:05:26: step  3650, loss 0.004, acc 0.968, std 0.035
2019/05/14 23:07:05: step  3700, loss 0.002, acc 0.964, std 0.035
2019/05/14 23:08:44: step  3750, loss 0.003, acc 0.966, std 0.034
2019/05/14 23:10:23: step  3800, loss 0.003, acc 0.969, std 0.032
2019/05/14 23:12:02: step  3850, loss 0.002, acc 0.974, std 0.037
2019/05/14 23:13:41: step  3900, loss 0.003, acc 0.971, std 0.034
2019/05/14 23:15:21: step  3950, loss 0.003, acc 0.979, std 0.036
2019/05/14 23:16:59: step  4000, loss 0.003, acc 0.965, std 0.036

learning_rate = 0.0001
2019/05/15 21:13:05: step     0, loss 1.386, acc 0.958, std 0.247
2019/05/15 21:14:46: step    50, loss 0.577, acc 0.936, std 0.178
2019/05/15 21:16:27: step   100, loss 0.191, acc 0.907, std 0.131
2019/05/15 21:18:07: step   150, loss 0.070, acc 0.877, std 0.094
2019/05/15 21:19:47: step   200, loss 0.042, acc 0.895, std 0.070
2019/05/15 21:21:26: step   250, loss 0.062, acc 0.903, std 0.060
2019/05/15 21:23:06: step   300, loss 0.024, acc 0.918, std 0.065
2019/05/15 21:24:45: step   350, loss 0.027, acc 0.921, std 0.055
2019/05/15 21:26:25: step   400, loss 0.024, acc 0.925, std 0.060
2019/05/15 21:28:04: step   450, loss 0.020, acc 0.923, std 0.055
2019/05/15 21:29:43: step   500, loss 0.024, acc 0.916, std 0.053
2019/05/15 21:31:23: step   550, loss 0.016, acc 0.924, std 0.054
2019/05/15 21:33:02: step   600, loss 0.017, acc 0.921, std 0.061
2019/05/15 21:34:41: step   650, loss 0.016, acc 0.931, std 0.060
2019/05/15 21:36:21: step   700, loss 0.017, acc 0.928, std 0.048
2019/05/15 21:38:00: step   750, loss 0.014, acc 0.939, std 0.047
2019/05/15 21:39:41: step   800, loss 0.020, acc 0.935, std 0.054
2019/05/15 21:41:20: step   850, loss 0.013, acc 0.938, std 0.051
2019/05/15 21:42:59: step   900, loss 0.015, acc 0.939, std 0.047
2019/05/15 21:44:39: step   950, loss 0.015, acc 0.932, std 0.046
2019/05/15 21:46:18: step  1000, loss 0.008, acc 0.938, std 0.052
2019/05/15 21:47:57: step  1050, loss 0.012, acc 0.937, std 0.056
2019/05/15 21:49:37: step  1100, loss 0.016, acc 0.935, std 0.053
2019/05/15 21:51:16: step  1150, loss 0.016, acc 0.933, std 0.061
2019/05/15 21:52:56: step  1200, loss 0.006, acc 0.933, std 0.059
2019/05/15 21:54:35: step  1250, loss 0.008, acc 0.924, std 0.055
2019/05/15 21:56:14: step  1300, loss 0.019, acc 0.932, std 0.055
2019/05/15 21:57:53: step  1350, loss 0.010, acc 0.936, std 0.054
2019/05/15 21:59:33: step  1400, loss 0.012, acc 0.940, std 0.054
2019/05/15 22:01:12: step  1450, loss 0.008, acc 0.932, std 0.058
2019/05/15 22:02:51: step  1500, loss 0.011, acc 0.934, std 0.060
2019/05/15 22:04:30: step  1550, loss 0.010, acc 0.925, std 0.059
2019/05/15 22:06:09: step  1600, loss 0.012, acc 0.937, std 0.062
2019/05/15 22:07:48: step  1650, loss 0.010, acc 0.937, std 0.064
2019/05/15 22:09:27: step  1700, loss 0.009, acc 0.923, std 0.061
2019/05/15 22:11:06: step  1750, loss 0.007, acc 0.930, std 0.061
2019/05/15 22:12:45: step  1800, loss 0.006, acc 0.926, std 0.063
2019/05/15 22:14:25: step  1850, loss 0.008, acc 0.931, std 0.062
2019/05/15 22:16:04: step  1900, loss 0.007, acc 0.929, std 0.062
2019/05/15 22:17:44: step  1950, loss 0.007, acc 0.924, std 0.062
2019/05/15 22:19:23: step  2000, loss 0.009, acc 0.926, std 0.064
2019/05/15 22:21:02: step  2050, loss 0.009, acc 0.928, std 0.063
2019/05/15 22:22:43: step  2100, loss 0.013, acc 0.925, std 0.066
2019/05/15 22:24:22: step  2150, loss 0.006, acc 0.921, std 0.063
2019/05/15 22:26:01: step  2200, loss 0.006, acc 0.927, std 0.063
2019/05/15 22:27:41: step  2250, loss 0.007, acc 0.932, std 0.062
2019/05/15 22:29:20: step  2300, loss 0.006, acc 0.926, std 0.066
2019/05/15 22:31:01: step  2350, loss 0.004, acc 0.918, std 0.065
2019/05/15 22:32:41: step  2400, loss 0.004, acc 0.912, std 0.067
2019/05/15 22:34:20: step  2450, loss 0.005, acc 0.921, std 0.065
2019/05/15 22:35:59: step  2500, loss 0.004, acc 0.929, std 0.069
2019/05/15 22:37:39: step  2550, loss 0.005, acc 0.921, std 0.067
2019/05/15 22:39:18: step  2600, loss 0.008, acc 0.918, std 0.073
2019/05/15 22:40:58: step  2650, loss 0.009, acc 0.915, std 0.066
2019/05/15 22:42:37: step  2700, loss 0.006, acc 0.922, std 0.070
2019/05/15 22:44:16: step  2750, loss 0.006, acc 0.914, std 0.070
2019/05/15 22:45:55: step  2800, loss 0.008, acc 0.912, std 0.068
2019/05/15 22:47:34: step  2850, loss 0.011, acc 0.921, std 0.064
2019/05/15 22:49:14: step  2900, loss 0.005, acc 0.918, std 0.064
2019/05/15 22:50:53: step  2950, loss 0.008, acc 0.914, std 0.063
2019/05/15 22:52:32: step  3000, loss 0.006, acc 0.916, std 0.068
2019/05/15 22:54:11: step  3050, loss 0.006, acc 0.918, std 0.068
2019/05/15 22:55:50: step  3100, loss 0.007, acc 0.915, std 0.067
2019/05/15 22:57:29: step  3150, loss 0.003, acc 0.916, std 0.069
2019/05/15 22:59:09: step  3200, loss 0.005, acc 0.918, std 0.069
2019/05/15 23:00:48: step  3250, loss 0.010, acc 0.915, std 0.065
2019/05/15 23:02:28: step  3300, loss 0.008, acc 0.908, std 0.066
2019/05/15 23:04:07: step  3350, loss 0.003, acc 0.910, std 0.068
2019/05/15 23:05:47: step  3400, loss 0.004, acc 0.907, std 0.070
2019/05/15 23:07:26: step  3450, loss 0.006, acc 0.916, std 0.071
2019/05/15 23:09:05: step  3500, loss 0.007, acc 0.915, std 0.073
2019/05/15 23:10:45: step  3550, loss 0.004, acc 0.918, std 0.073
2019/05/15 23:12:24: step  3600, loss 0.005, acc 0.911, std 0.073
2019/05/15 23:14:03: step  3650, loss 0.006, acc 0.910, std 0.072
2019/05/15 23:15:43: step  3700, loss 0.003, acc 0.920, std 0.073
2019/05/15 23:17:23: step  3750, loss 0.006, acc 0.918, std 0.069
2019/05/15 23:19:03: step  3800, loss 0.006, acc 0.917, std 0.070
2019/05/15 23:20:43: step  3850, loss 0.006, acc 0.912, std 0.070
2019/05/15 23:22:23: step  3900, loss 0.004, acc 0.912, std 0.069
2019/05/15 23:24:03: step  3950, loss 0.002, acc 0.915, std 0.070
2019/05/15 23:25:42: step  4000, loss 0.005, acc 0.918, std 0.073

