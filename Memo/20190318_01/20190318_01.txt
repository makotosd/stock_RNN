
debug中の一時的な高速化のためにbatch sizeを64→4にしたら
精度もよくなっている。なんでだろ。step8500なんて、1%切った。

ところが8500で切った1%エラーのモデルで、testしてみると<キャプチャ.png>のように
全然だめ、汎化能力が全然足りない。

(tensorflow) C:\Users\Makoto\PycharmProjects\stock_RNN>python train.py --cc 1330 6701 6702 --target_feature 6701_close
cc: 1330,6701,6702
feature: open,close,high,low,highopen
quote:

2019/03/19 01:24:15: step 0, error 0.963
2019/03/19 01:24:45: step 500, error 0.143
2019/03/19 01:25:18: step 1000, error 0.069
2019/03/19 01:25:53: step 1500, error 0.088
2019/03/19 01:26:26: step 2000, error 0.050
2019/03/19 01:27:10: step 2500, error 0.030
2019/03/19 01:27:39: step 3000, error 0.012
2019/03/19 01:28:12: step 3500, error 0.034
2019/03/19 01:28:43: step 4000, error 0.068
2019/03/19 01:29:16: step 4500, error 0.030
2019/03/19 01:29:53: step 5000, error 0.033
2019/03/19 01:30:26: step 5500, error 0.027
2019/03/19 01:31:00: step 6000, error 0.025
2019/03/19 01:31:39: step 6500, error 0.030
2019/03/19 01:32:16: step 7000, error 0.021
2019/03/19 01:32:48: step 7500, error 0.031
2019/03/19 01:33:18: step 8000, error 0.044
2019/03/19 01:33:45: step 8500, error 0.007
2019/03/19 01:34:12: step 9000, error 0.024
2019/03/19 01:34:40: step 9500, error 0.045